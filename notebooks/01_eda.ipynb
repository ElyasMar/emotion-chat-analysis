{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e0e6f-44f8-4584-8fba-c7b0380e7f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('../data/train_processed.csv')\n",
    "\n",
    "# 1. DATASET OVERVIEW\n",
    "print(\"=\"*50)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total samples: {len(train_df)}\")\n",
    "print(f\"\\nShape: {train_df.shape}\")\n",
    "print(f\"\\nColumns: {train_df.columns.tolist()}\")\n",
    "print(f\"\\nMissing values:\\n{train_df.isnull().sum()}\")\n",
    "print(f\"\\nData types:\\n{train_df.dtypes}\")\n",
    "\n",
    "# 2. TEXT STATISTICS\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEXT STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "print(train_df[['text_length', 'word_count']].describe())\n",
    "\n",
    "# Visualize text length distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(train_df['text_length'], bins=50, edgecolor='black')\n",
    "axes[0].set_title('Distribution of Text Length (characters)')\n",
    "axes[0].set_xlabel('Character Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].hist(train_df['word_count'], bins=50, edgecolor='black', color='coral')\n",
    "axes[1].set_title('Distribution of Word Count')\n",
    "axes[1].set_xlabel('Word Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/text_length_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# 3. EMOTION DISTRIBUTION (TARGET VARIABLE ANALYSIS)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EMOTION DISTRIBUTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "emotion_counts = train_df['primary_emotion_name'].value_counts()\n",
    "print(emotion_counts)\n",
    "\n",
    "# Visualize emotion distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "emotion_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Emotions in Training Data', fontsize=16)\n",
    "plt.xlabel('Emotion', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/emotion_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# Check for class imbalance\n",
    "print(f\"\\nClass imbalance ratio (max/min): {emotion_counts.max() / emotion_counts.min():.2f}\")\n",
    "\n",
    "# 4. WORD FREQUENCY ANALYSIS\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"WORD FREQUENCY ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get all words\n",
    "all_text = ' '.join(train_df['text'].astype(str))\n",
    "words = all_text.lower().split()\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words_filtered = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "\n",
    "# Most common words\n",
    "word_freq = Counter(words_filtered)\n",
    "print(\"\\nTop 20 most common words:\")\n",
    "for word, count in word_freq.most_common(20):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# 5. WORD CLOUDS BY EMOTION\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GENERATING WORD CLOUDS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Select top 6 emotions for word clouds\n",
    "top_emotions = emotion_counts.head(6).index\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, emotion in enumerate(top_emotions):\n",
    "    emotion_text = ' '.join(train_df[train_df['primary_emotion_name'] == emotion]['text'].astype(str))\n",
    "    \n",
    "    wordcloud = WordCloud(width=400, height=300, \n",
    "                         background_color='white',\n",
    "                         stopwords=stop_words,\n",
    "                         max_words=50).generate(emotion_text)\n",
    "    \n",
    "    axes[idx].imshow(wordcloud, interpolation='bilinear')\n",
    "    axes[idx].set_title(f'{emotion.upper()}', fontsize=14, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/emotion_wordclouds.png')\n",
    "plt.show()\n",
    "\n",
    "# 6. TEXT LENGTH BY EMOTION\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEXT LENGTH BY EMOTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "train_df.boxplot(column='text_length', by='primary_emotion_name', figsize=(14, 6))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Text Length Distribution by Emotion')\n",
    "plt.suptitle('')\n",
    "plt.ylabel('Character Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/text_length_by_emotion.png')\n",
    "plt.show()\n",
    "\n",
    "# 7. CORRELATION ANALYSIS\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Numeric features correlation\n",
    "numeric_features = ['text_length', 'word_count', 'primary_emotion']\n",
    "correlation_matrix = train_df[numeric_features].corr()\n",
    "print(correlation_matrix)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebooks/correlation_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# 8. SAMPLE TEXTS BY EMOTION\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAMPLE TEXTS BY EMOTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for emotion in top_emotions[:3]:\n",
    "    print(f\"\\n{emotion.upper()}:\")\n",
    "    samples = train_df[train_df['primary_emotion_name'] == emotion]['text'].head(3).tolist()\n",
    "    for i, sample in enumerate(samples, 1):\n",
    "        print(f\"  {i}. {sample}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EDA COMPLETE\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
