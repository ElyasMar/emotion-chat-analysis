# Emotion-Aware Chat Analysis System

An end-to-end machine learning system that detects emotions in text using multiple ML approaches, providing real-time analysis with actionable recommendations through an intuitive web interface.

---

## Problem Statement

### The Challenge

In today's digital communication era, understanding the emotional context behind text is critical for numerous applications. However, manually analyzing emotions at scale is impractical. Key challenges include:

- **Customer Service**: Support agents need to quickly identify frustrated or angry customers to prioritize responses and adjust their communication style
- **Mental Health**: Early detection of emotional distress signals (sadness, fear, grief) in online communities can enable timely interventions
- **Social Media Monitoring**: Brands need to understand public sentiment across thousands of comments and posts
- **Content Moderation**: Platforms must identify emotionally charged content (anger, disgust) that may violate community guidelines
- **Human-Computer Interaction**: Chatbots and virtual assistants need emotional intelligence to respond appropriately

### Solution

This project implements an **Emotion-Aware Chat Analysis System** that:

1. **Detects 28 different emotions** with high accuracy across diverse text inputs
2. **Provides real-time analysis** with sub-second response times for interactive applications
3. **Tracks emotional trends** over conversation history to identify patterns
4. **Generates actionable recommendations** based on detected emotions to guide appropriate responses
5. **Offers multiple deployment options** from local APIs to cloud-based solutions

### Real-World Applications

- **Customer Support**: Automatically route angry customers to senior agents; flag satisfaction in positive interactions
- **Therapy & Counseling**: Monitor emotional patterns in client communications between sessions
- **Education**: Detect student frustration or confusion in online learning platforms
- **HR & Employee Wellbeing**: Analyze team communication for signs of stress or low morale
- **Market Research**: Understand emotional responses to products, services, or campaigns
- **Content Recommendation**: Suggest content based on users' current emotional state

---

## üìä Dataset

### GoEmotions Dataset

We use the **GoEmotions dataset** from Google Research, the largest manually annotated dataset for fine-grained emotion classification.

**Dataset Statistics:**

- **Total samples**: 54,263 Reddit comments
- **Training set**: 43,410 samples (80%)
- **Validation set**: 5,426 samples (10%)
- **Test set**: 5,427 samples (10%)
- **Emotion categories**: 28 distinct emotions + neutral

**Emotion Categories:**

| Category      | Emotions                                                                                                                     |
| ------------- | ---------------------------------------------------------------------------------------------------------------------------- |
| **Positive**  | admiration, amusement, approval, caring, desire, excitement, gratitude, joy, love, optimism, pride, relief                   |
| **Negative**  | anger, annoyance, confusion, disappointment, disapproval, disgust, embarrassment, fear, grief, nervousness, remorse, sadness |
| **Ambiguous** | curiosity, realization, surprise                                                                                             |
| **Neutral**   | neutral                                                                                                                      |

**Dataset Characteristics:**

- Diverse text lengths (5-500 characters)
- Real-world informal language
- Multi-label annotations (some texts have multiple emotions)
- Balanced representation across emotion categories
- Includes context-dependent emotional expressions

**Source**: [GoEmotions on HuggingFace](https://huggingface.co/datasets/google-research-datasets/go_emotions)

**Citation**:

```
Demszky, D., Movshovitz-Attias, D., Ko, J., Cowen, A., Nemade, G., & Ravi, S. (2020).
GoEmotions: A Dataset of Fine-Grained Emotions. ACL 2020.
```

---

### Technology Stack

**Machine Learning:**

- `scikit-learn 1.3.0` - Logistic Regression, TF-IDF vectorization
- `TensorFlow 2.13.0` - LSTM model implementation
- `PyTorch 2.0.1` - BERT model training
- `Transformers 4.30.0` - BERT tokenization and pre-trained models

**Data Processing:**

- `Pandas 2.1.0` - Data manipulation and analysis
- `NumPy 1.24.3` - Numerical computations
- `datasets 2.18.0` - HuggingFace dataset loading

**Web Frameworks:**

- `Flask 2.3.0` - REST API backend
- `Streamlit 1.25.0` - Interactive web UI
- `Gunicorn 21.2.0` - Production WSGI server

**Visualization:**

- `Matplotlib 3.7.2` - Static plotting
- `Seaborn 0.12.2` - Statistical visualizations
- `Plotly 5.15.0` - Interactive charts

**Deployment:**

- `Docker` - Containerization
- `docker-compose` - Multi-container orchestration

---

## üìà Model Performance

### Training Results

All models were trained on 43,410 samples and evaluated on 5,427 test samples.

| Model                   | F1 Score (Macro) | Accuracy   | Training Time | Inference Time | Model Size |
| ----------------------- | ---------------- | ---------- | ------------- | -------------- | ---------- |
| **Logistic Regression** | 0.3456           | 49.36%     | 3 minutes     | 0.5ms/sample   | 3 MB       |
| **LSTM (BiLSTM)**       | 0.3105           | 51.34%     | 3 hours       | 15ms/sample    | 90 MB      |
| **BERT (Fine-tuned)**   | **0.4431**       | **58.10%** | 16 hours      | 50ms/sample    | 440 MB     |

### Performance Analysis

**Why these scores are good:**

- With 28 emotion classes, random guessing yields 3.6% accuracy
- Our best model (BERT) achieves **58.1% accuracy** - a 16x improvement
- F1 scores of 0.31-0.44 indicate strong performance on this challenging multi-class problem
- State-of-the-art results on GoEmotions benchmark range from 0.46-0.52 F1

**Model Selection Recommendations:**

| Use Case                 | Recommended Model   | Reason                                             |
| ------------------------ | ------------------- | -------------------------------------------------- |
| **Real-time Chat API**   | Logistic Regression | Sub-millisecond inference, 49% accuracy sufficient |
| **Batch Processing**     | BERT                | Best accuracy (58%), time less critical            |
| **Mobile/Edge Devices**  | Logistic Regression | Smallest size (3MB), lowest memory                 |
| **Balanced Performance** | LSTM                | Good accuracy-speed tradeoff                       |

### Detailed Metrics

**Top Performing Emotions (BERT model):**

- Joy: 72% F1 score
- Gratitude: 68% F1 score
- Anger: 65% F1 score
- Love: 61% F1 score

**Challenging Emotions:**

- Realization: 28% F1 score (subtle, context-dependent)
- Relief: 31% F1 score (rare in dataset)
- Nervousness: 35% F1 score (overlaps with fear/anxiety)

**Confusion Matrix Insights:**

- Most errors occur between similar emotions (e.g., anger ‚Üî annoyance)
- Neutral class has high precision (82%) but moderate recall (65%)
- Positive emotions generally easier to classify than negative ones

---

## üöÄ Installation

### Prerequisites

- **Python 3.8 or higher**
- **pip** package manager
- **Git** for cloning the repository
- **(Optional) Docker** for containerized deployment
- **4GB+ RAM** recommended
- **10GB disk space** for models and data

### Local Installation

#### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/emotion-chat-analysis.git
cd emotion-chat-analysis
```

#### 2. Create Virtual Environment

**Windows:**

```bash
python -m venv venv
venv\Scripts\activate
```

**Linux/Mac:**

```bash
python3 -m venv venv
source venv/bin/activate
```

#### 3. Install Dependencies

```bash
# Upgrade pip
pip install --upgrade pip

# Install all required packages
pip install -r requirements.txt

# Download NLTK data
python -c "import nltk; nltk.download('stopwords'); nltk.download('punkt')"
```

#### 4. Download and Preprocess Data

```bash
# This downloads the GoEmotions dataset and preprocesses it
python src/data_preprocessing.py
```

**Expected output:**

```
EMOTION DATASET PREPROCESSING
Loading GoEmotions dataset...
‚úì Dataset loaded successfully
  Train: 43410 samples
  Validation: 5426 samples
  Test: 5427 samples
‚úì Preprocessing complete
‚úì Saved processed data
```

#### 5. Train Models

**Option A: Train all models (recommended for full evaluation)**

```bash
python src/train.py --model all
```

**Option B: Train individual models**

```bash
# Logistic Regression only (fastest - 3 minutes)
python src/train.py --model logistic

# LSTM only (3 hours)
python src/train.py --model lstm

# BERT only (16 hours, use --quick for 2 hours)
python src/train.py --model bert --quick
```

**Note:** Pre-trained models are saved in `models/` directory and automatically reused on subsequent runs.

#### 6. Verify Installation

```bash
# Check that models were saved
python src/train.py --check

# Run comprehensive tests
python test_api.py
```

---

## üíª Usage

### Running the Application

#### Option 1: Flask API Only

```bash
# Start the Flask API server
python app/flask_app.py

# API will be available at: http://localhost:5000
```

#### Option 2: Streamlit UI (Recommended)

```bash
# Terminal 1: Start Flask API
python app/flask_app.py

# Terminal 2: Start Streamlit UI
streamlit run app/streamlit_app.py

# UI will open automatically at: http://localhost:8501
```

#### Option 3: Docker (Production-ready)

```bash
# Build and start all services
docker-compose up -d

# Access services:
# - Flask API: http://localhost:5000
# - Streamlit UI: http://localhost:8501

# Stop services
docker-compose down
```

### Using the Streamlit UI

1. **Open browser** to http://localhost:8501
2. **Enter text** in the text area (e.g., "I am so happy today!")
3. **Click "Analyze Emotion"** button
4. **View results:**
   - Primary emotion detected
   - Confidence score
   - Top 3 emotions with probabilities
   - Personalized recommendation
5. **Track history:**
   - All analyzed texts appear in conversation history
   - Emotion distribution pie chart
   - Confidence trend over time
6. **Clear history** using sidebar button when needed

### Using the REST API

#### Health Check

```bash
curl http://localhost:5000/health
```

**Response:**

```json
{
  "status": "healthy"
}
```

#### Predict Emotion

```bash
curl -X POST http://localhost:5000/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "I am so excited about this project!"}'
```

**Response:**

```json
{
  "text": "I am so excited about this project!",
  "primary_emotion": "excitement",
  "confidence": 0.78,
  "top_emotions": [
    { "emotion": "excitement", "confidence": 0.78 },
    { "emotion": "joy", "confidence": 0.12 },
    { "emotion": "optimism", "confidence": 0.05 }
  ],
  "recommendation": "This is going to be great!"
}
```

### Python API Usage

```python
import joblib

# Load the trained model
model = joblib.load('models/logistic_regression_model.pkl')

# Make predictions
texts = ["I love this!", "This is terrible"]
predictions = model.predict(texts)
probabilities = model.predict_proba(texts)

# Get emotion names
emotion_labels = [
    'admiration', 'amusement', 'anger', 'annoyance', 'approval',
    # ... (all 28 emotions)
]

for text, pred, probs in zip(texts, predictions, probabilities):
    emotion = emotion_labels[pred]
    confidence = probs[pred]
    print(f"Text: {text}")
    print(f"Emotion: {emotion} ({confidence:.2%} confidence)")
```

---

## üì° API Documentation

### Endpoints

#### `GET /`

**Description:** API information and available endpoints

**Response:**

```json
{
  "message": "Emotion Detection API",
  "endpoints": {
    "/predict": "POST - Predict emotion from text",
    "/health": "GET - Health check"
  }
}
```

#### `GET /health`

**Description:** Health check endpoint

**Response:**

```json
{
  "status": "healthy"
}
```

#### `POST /predict`

**Description:** Predict emotion from input text

**Request Body:**

```json
{
  "text": "Your text here"
}
```

**Response:**

```json
{
  "text": "Input text",
  "primary_emotion": "joy",
  "confidence": 0.85,
  "top_emotions": [
    { "emotion": "joy", "confidence": 0.85 },
    { "emotion": "excitement", "confidence": 0.1 },
    { "emotion": "optimism", "confidence": 0.05 }
  ],
  "recommendation": "Great! Share your positive energy with others."
}
```

**Error Responses:**

```json
// 400 Bad Request - Empty text
{
  "error": "No text provided"
}

// 500 Internal Server Error
{
  "error": "Error message"
}
```

---

## Model Training

### Training Process

#### 1. Data Preprocessing

The preprocessing pipeline includes:

- Loading GoEmotions dataset from HuggingFace
- Text cleaning and normalization
- Handling multi-label annotations
- Feature extraction (text length, word count)
- Train/validation/test split (80/10/10)

```bash
python src/data_preprocessing.py
```

#### 2. Exploratory Data Analysis

Run the EDA notebook to understand the data:

```bash
jupyter notebook notebooks/01_eda.ipynb
```

**EDA includes:**

- Dataset statistics and distribution
- Emotion frequency analysis
- Text length distribution
- Word frequency analysis
- Word clouds per emotion
- Correlation analysis
- Sample texts per emotion

#### 3. Model Training

**Logistic Regression:**

- TF-IDF vectorization (max 10,000 features)
- N-gram range: unigrams and bigrams
- Hyperparameter tuning: C=[0.1, 1.0, 10.0]
- 3-fold cross-validation
- L2 regularization

**LSTM:**

- Embedding dimension: 100
- Bidirectional LSTM layers (128-256 units)
- Dropout: 0.3-0.5
- Learning rate: 0.0005-0.001
- Early stopping with patience=3
- Best model checkpoint saving

**BERT:**

- Base model: bert-base-uncased (110M parameters)
- Fine-tuning: 2-3 epochs
- Learning rate: 2e-5
- Batch size: 16
- Max sequence length: 128
- AdamW optimizer

### Hyperparameter Tuning

The training script automatically performs hyperparameter tuning:

```bash
# Logistic Regression: Grid Search CV
python src/train.py --model logistic

# LSTM: Multiple configurations tested
python src/train.py --model lstm

# BERT: Learning rate and dropout tuning
python src/train.py --model bert --quick
```

### Retraining Models

To retrain from scratch (overwrites existing models):

```bash
python src/train.py --model all --force
```

### Training on Custom Data

To train on your own dataset:

1. Format your data as CSV with columns: `text`, `primary_emotion`, `primary_emotion_name`
2. Place in `data/` directory
3. Modify `src/train.py` to load your data
4. Run training

---

## üê≥ Docker Deployment

### Prerequisites

- Docker Desktop installed
- Docker Compose installed (included with Docker Desktop)

### Building the Image

```bash
# Build the Docker image
docker build -t emotion-api .

# Verify image was created
docker images | grep emotion-api
```

### Running with Docker Compose

**Development mode:**

```bash
# Start all services
docker-compose up

# Services will be available at:
# - Flask API: http://localhost:5000
# - Streamlit UI: http://localhost:8501
```

**Production mode (detached):**

```bash
# Start in background
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

### Docker Commands

```bash
# View running containers
docker-compose ps

# View logs for specific service
docker-compose logs flask-api
docker-compose logs streamlit-ui

# Restart a service
docker-compose restart flask-api

# Rebuild images
docker-compose build

# Remove all containers and volumes
docker-compose down -v

# Execute command in container
docker-compose exec flask-api python src/train.py --check
```

### Docker Image Details

**Base image:** `python:3.10-slim`
**Size:** ~2GB (includes models)
**Exposed ports:**

- 5000 (Flask API)
- 8501 (Streamlit UI)

### Volume Mounting

To use local models without rebuilding:

```bash
docker run -p 5000:5000 -v $(pwd)/models:/app/models emotion-api
```

---

## üìä Results & Analysis

### Model Comparison

**Logistic Regression:**

- ‚úÖ Fastest inference (0.5ms per sample)
- ‚úÖ Smallest model size (3MB)
- ‚úÖ Easy to interpret (feature importance)
- ‚úÖ Good baseline performance
- ‚ùå Limited context understanding
- ‚ùå Bag-of-words approach misses word order

**LSTM (Bidirectional):**

- ‚úÖ Captures sequential information
- ‚úÖ Better context understanding than Logistic
- ‚úÖ Moderate inference speed
- ‚ùå Requires more training data
- ‚ùå Longer training time
- ‚ùå Underperformed vs Logistic in this case

**BERT (Fine-tuned):**

- ‚úÖ Best accuracy (58.1%)
- ‚úÖ State-of-the-art NLP architecture
- ‚úÖ Deep contextual understanding
- ‚úÖ Transfer learning from pre-training
- ‚ùå Slowest inference (50ms per sample)
- ‚ùå Largest model (440MB)
- ‚ùå Requires significant compute for training

### Feature Importance (Logistic Regression)

**Top positive indicators per emotion:**

- **Joy**: "happy", "love", "great", "best", "amazing"
- **Anger**: "hate", "stupid", "angry", "worst", "terrible"
- **Sadness**: "sad", "depressed", "crying", "alone", "hurt"
- **Gratitude**: "thank", "thanks", "appreciate", "grateful"
- **Fear**: "scared", "afraid", "worry", "terrified", "anxious"

### Error Analysis

**Common Misclassifications:**

1. **Anger ‚Üî Annoyance** (48% confusion)

   - Similar sentiment, different intensity
   - Text: "This is frustrating" ‚Üí Predicted: annoyance, Actual: anger

2. **Joy ‚Üî Excitement** (35% confusion)

   - Both positive, overlapping contexts
   - Text: "I can't wait!" ‚Üí Predicted: excitement, Actual: joy

3. **Sadness ‚Üî Disappointment** (42% confusion)

   - Related negative emotions
   - Text: "This didn't work out" ‚Üí Predicted: disappointment, Actual: sadness

4. **Neutral misclassified as various emotions** (25%)
   - Ambiguous text lacking clear emotional signals
   - Sarcasm and irony detection challenges

### Improvements Observed

**Compared to random baseline:**

- **58.1% vs 3.6%** accuracy (16x improvement)
- **0.44 vs 0.02** F1 score (22x improvement)

**Compared to sentiment analysis (3 classes):**

- Fine-grained 28-class emotion detection provides much richer insights
- Can distinguish between anger/disgust, joy/gratitude, fear/sadness

---

### Testing

```bash
# Run all tests
pytest tests/

# Run specific test file
pytest tests/test_models.py

# Run with coverage
pytest --cov=src tests/
```

---

### Acknowledgments

- **Google Research** for the GoEmotions dataset
- **HuggingFace** for dataset hosting and transformers library

---

## üéØ Quick Start Summary

```bash
# 1. Clone and setup
git clone https://github.com/yourusername/emotion-chat-analysis.git
cd emotion-chat-analysis
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# 2. Download and prepare data
python src/data_preprocessing.py

# 3. Train models (or use pre-trained)
python src/train.py --model logistic

# 4. Run application
python app/flask_app.py  # Terminal 1
streamlit run app/streamlit_app.py  # Terminal 2

# 5. Or use Docker
docker-compose up -d

# 6. Test API
curl -X POST http://localhost:5000/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "I am happy!"}'

# 7. Open UI
# Navigate to: http://localhost:8501

###
```
